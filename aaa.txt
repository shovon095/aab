g: 0.0772, loss_cls: 0.2675, acc: 91.9277, loss_bbox: 0.2653, loss_con: 0.0000, loss: 0.8678
2025-06-25 23:53:34,896 - mmdet - INFO - Epoch [1][1100/90421]  lr: 1.000e-02, eta: 2 days, 22:06:11, time: 0.227, data_time: 0.004, memory: 2882, s0.loss_rpn_reg: 0.2683, s1.loss_rpn_cls: 0.0194, s1.loss_rpn_reg: 0.0957, loss_cls: 0.2837, acc: 90.7734, loss_bbox: 0.3155, loss_con: 0.0000, loss: 0.9826
Traceback (most recent call last):
  File "tools/train.py", line 247, in <module>
    main()
  File "tools/train.py", line 236, in main
    train_detector(
  File "/home/shouvon/CFINet/mmdet/apis/train.py", line 246, in train_detector
    runner.run(data_loaders, cfg.workflow)
  File "/home/shouvon/miniconda3/envs/cfinet/lib/python3.8/site-packages/mmcv/runner/epoch_based_runner.py", line 127, in run
    epoch_runner(data_loaders[i], **kwargs)
  File "/home/shouvon/miniconda3/envs/cfinet/lib/python3.8/site-packages/mmcv/runner/epoch_based_runner.py", line 50, in train
    self.run_iter(data_batch, train_mode=True, **kwargs)
  File "/home/shouvon/miniconda3/envs/cfinet/lib/python3.8/site-packages/mmcv/runner/epoch_based_runner.py", line 29, in run_iter
    outputs = self.model.train_step(data_batch, self.optimizer,
  File "/home/shouvon/miniconda3/envs/cfinet/lib/python3.8/site-packages/mmcv/parallel/distributed.py", line 59, in train_step
    output = self.module.train_step(*inputs[0], **kwargs[0])
  File "/home/shouvon/CFINet/mmdet/models/detectors/base.py", line 248, in train_step
    losses = self(**data)
  File "/home/shouvon/miniconda3/envs/cfinet/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/shouvon/miniconda3/envs/cfinet/lib/python3.8/site-packages/mmcv/runner/fp16_utils.py", line 140, in new_func
    output = old_func(*new_args, **new_kwargs)
  File "/home/shouvon/CFINet/mmdet/models/detectors/base.py", line 172, in forward
    return self.forward_train(img, img_metas, **kwargs)
  File "/home/shouvon/CFINet/mmdet/models/detectors/two_stage.py", line 147, in forward_train
    roi_losses = self.roi_head.forward_train(x, img_metas, proposal_list,
  File "/home/shouvon/CFINet/mmdet/models/roi_heads/feature_imitation_roi_head.py", line 204, in forward_train
    bbox_results = self._bbox_forward_train(
  File "/home/shouvon/CFINet/mmdet/models/roi_heads/feature_imitation_roi_head.py", line 436, in _bbox_forward_train
    self._update_iq_score_info(cur_gt_cat_id.item(), cur_gt_roi_feat)
  File "/home/shouvon/CFINet/mmdet/models/roi_heads/feature_imitation_roi_head.py", line 300, in _update_iq_score_info
    torch.save(
  File "/home/shouvon/miniconda3/envs/cfinet/lib/python3.8/site-packages/torch/serialization.py", line 376, in save
    with _open_file_like(f, 'wb') as opened_file:
  File "/home/shouvon/miniconda3/envs/cfinet/lib/python3.8/site-packages/torch/serialization.py", line 230, in _open_file_like
    return _open_file(name_or_buffer, mode)
  File "/home/shouvon/miniconda3/envs/cfinet/lib/python3.8/site-packages/torch/serialization.py", line 211, in __init__
    super(_open_file, self).__init__(open(name, mode))
FileNotFoundError: [Errno 2] No such file or directory: './work_dirs/roi_feats/cfinet/1/6.pt'
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 20737 closing signal SIGTERM
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 20739 closing signal SIGTERM
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 20740 closing signal SIGTERM
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 1 (pid: 20738) of binary: /home/shouvon/miniconda3/envs/cfinet/bin/python
Traceback (most recent call last):
  File "/home/shouvon/miniconda3/envs/cfinet/lib/python3.8/runpy.py", line 194, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/home/shouvon/miniconda3/envs/cfinet/lib/python3.8/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/home/shouvon/miniconda3/envs/cfinet/lib/python3.8/site-packages/torch/distributed/launch.py", line 193, in <module>
    main()
  File "/home/shouvon/miniconda3/envs/cfinet/lib/python3.8/site-packages/torch/distributed/launch.py", line 189, in main
    launch(args)
  File "/home/shouvon/miniconda3/envs/cfinet/lib/python3.8/site-packages/torch/distributed/launch.py", line 174, in launch
    run(args)
  File "/home/shouvon/miniconda3/envs/cfinet/lib/python3.8/site-packages/torch/distributed/run.py", line 710, in run
    elastic_launch(
  File "/home/shouvon/miniconda3/envs/cfinet/lib/python3.8/site-packages/torch/distributed/launcher/api.py", line 131, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/home/shouvon/miniconda3/envs/cfinet/lib/python3.8/site-packages/torch/distributed/launcher/api.py", line 259, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError:
============================================================
tools/train.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2025-06-25_23:54:02
  host      : dxs4-DGX-Station
  rank      : 1 (local_rank: 1)
  exitcode  : 1 (pid: 20738)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
